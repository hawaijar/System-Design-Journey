# Day 2 - October 25, 2024

## Topics Covered
- Consistent Hashing algorithm and implementation
- Stateless vs Stateful architecture patterns
- Load balancer types and deployment strategies

## Key Learnings
- Consistent hashing minimizes key redistribution when nodes are added/removed
- Stateless architectures scale horizontally much easier than stateful ones
- Different load balancer layers serve different purposes (L4 vs L7)
- Virtual nodes help distribute load evenly in consistent hashing

---

## 1. Consistent Hashing

### The Problem: Traditional Hashing

```mermaid
graph TB
    subgraph "Simple Hash: key % num_servers"
        K1[Key: user_1234<br/>Hash % 3 = 0] --> S0[Server 0]
        K2[Key: user_5678<br/>Hash % 3 = 1] --> S1[Server 1]
        K3[Key: user_9012<br/>Hash % 3 = 2] --> S2[Server 2]
    end

    subgraph "Problem: Add Server 3"
        K1b[Key: user_1234<br/>Hash % 4 = 0] --> S0b[Server 0]
        K2b[Key: user_5678<br/>Hash % 4 = 1] --> S1b[Server 1]
        K3b[Key: user_9012<br/>Hash % 4 = 0] -.->|MOVED!| S0b
    end

    style K3b fill:#f99
    style S0b fill:#f99
```

**Issue:** When servers change, most keys get remapped ‚Üí cache invalidation storm!

### Consistent Hashing Solution

**Core Idea:** Hash both keys AND servers onto a ring (0 to 2^32-1)

```mermaid
graph TB
    subgraph "Hash Ring Concept"
        Ring[("Hash Ring<br/>0 to 2¬≥¬≤-1")]

        S0[Server 0<br/>Hash: 1000] --> Ring
        S1[Server 1<br/>Hash: 5000] --> Ring
        S2[Server 2<br/>Hash: 9000] --> Ring

        K1[Key A<br/>Hash: 2000] -.->|Clockwise to<br/>next server| S1
        K2[Key B<br/>Hash: 6000] -.->|Clockwise to<br/>next server| S2
        K3[Key C<br/>Hash: 500] -.->|Clockwise to<br/>next server| S0
    end

    style Ring fill:#9cf
```

### Visual: Hash Ring

```mermaid
graph TD
    subgraph "Consistent Hash Ring"
        direction LR

        Start([0]) --> S0[Server 0<br/>Position: 1000]
        S0 --> K1[Key: user_A<br/>2000]
        K1 --> K2[Key: user_B<br/>3500]
        K2 --> S1[Server 1<br/>Position: 5000]
        S1 --> K3[Key: user_C<br/>6500]
        K3 --> S2[Server 2<br/>Position: 9000]
        S2 --> K4[Key: user_D<br/>10000]
        K4 --> End([Back to 0...])

        End -.-> Start
    end

    style S0 fill:#9f9
    style S1 fill:#9f9
    style S2 fill:#9f9
```

**Rule:** Each key goes to the first server clockwise from its hash position

### Adding a Server

```mermaid
graph TB
    subgraph "Before: 3 Servers"
        B0[Server 0: 1000]
        B1[Server 1: 5000]
        B2[Server 2: 9000]

        BK1[Keys 1001-5000<br/>6 keys] --> B1
        BK2[Keys 5001-9000<br/>5 keys] --> B2
        BK3[Keys 9001-1000<br/>7 keys] --> B0
    end

    subgraph "After: Add Server 3 at 7000"
        A0[Server 0: 1000]
        A1[Server 1: 5000]
        A3[Server 3: 7000<br/>NEW]
        A2[Server 2: 9000]

        AK1[Keys 1001-5000<br/>6 keys] --> A1
        AK2[Keys 5001-7000<br/>2 keys] --> A3
        AK3[Keys 7001-9000<br/>3 keys] --> A2
        AK4[Keys 9001-1000<br/>7 keys] --> A0
    end

    BK2 -.->|Only 2 keys moved!| AK2

    style A3 fill:#9f9
    style BK2 fill:#ff9
    style AK2 fill:#9f9
```

**Benefit:** Only keys between Server 1 (5000) and Server 3 (7000) are remapped!

### The Virtual Nodes Problem

**Problem: Without virtual nodes, servers may get uneven load**

```mermaid
graph LR
    subgraph Uneven["‚ùå Problem: Uneven Distribution"]
        P1[Server 0<br/>Pos: 1000<br/>Load: 90%]
        P2[Server 1<br/>Pos: 2000<br/>Load: 5%]
        P3[Server 2<br/>Pos: 3000<br/>Load: 5%]
    end

    style P1 fill:#f99
    style P2 fill:#ff9
    style P3 fill:#ff9
```

**Solution: Virtual Nodes - Distribute each server across multiple ring positions**

### Virtual Nodes Ring Visualization

```mermaid
graph TD
    Start([Ring Start: 0])

    Start --> S0V1[üü¢ S0-VNode1<br/>Position: 1000]
    S0V1 --> K1[üîë Key: user_alice<br/>Hash: 1500]
    K1 --> S1V1[üîµ S1-VNode1<br/>Position: 2000]
    S1V1 --> K2[üîë Key: user_bob<br/>Hash: 2800]
    K2 --> S2V1[üü† S2-VNode1<br/>Position: 3000]
    S2V1 --> K3[üîë Key: user_charlie<br/>Hash: 4200]
    K3 --> S0V2[üü¢ S0-VNode2<br/>Position: 5000]
    S0V2 --> K4[üîë Key: user_dave<br/>Hash: 5500]
    K4 --> S1V2[üîµ S1-VNode2<br/>Position: 6000]
    S1V2 --> K5[üîë Key: user_eve<br/>Hash: 6800]
    K5 --> S2V2[üü† S2-VNode2<br/>Position: 7000]
    S2V2 --> K6[üîë Key: user_frank<br/>Hash: 8500]
    K6 --> S0V3[üü¢ S0-VNode3<br/>Position: 9000]
    S0V3 --> K7[üîë Key: user_grace<br/>Hash: 9500]
    K7 --> S1V3[üîµ S1-VNode3<br/>Position: 10000]
    S1V3 --> K8[üîë Key: user_henry<br/>Hash: 10500]
    K8 --> S2V3[üü† S2-VNode3<br/>Position: 11000]
    S2V3 --> End([Wraps to 0...])

    End -.-> Start

    style S0V1 fill:#9f9
    style S0V2 fill:#9f9
    style S0V3 fill:#9f9
    style S1V1 fill:#9cf
    style S1V2 fill:#9cf
    style S1V3 fill:#9cf
    style S2V1 fill:#fc9
    style S2V2 fill:#fc9
    style S2V3 fill:#fc9
    style K1 fill:#ffd
    style K2 fill:#ffd
    style K3 fill:#ffd
    style K4 fill:#ffd
    style K5 fill:#ffd
    style K6 fill:#ffd
    style K7 fill:#ffd
    style K8 fill:#ffd
```

**Key Mapping Rules:**
- üîë **user_alice** (1500) ‚Üí Next server clockwise: üü¢ S0-VNode1 (2000) ‚Üí **Server 0**
- üîë **user_bob** (2800) ‚Üí Next server clockwise: üü† S2-VNode1 (3000) ‚Üí **Server 2**
- üîë **user_charlie** (4200) ‚Üí Next server clockwise: üü¢ S0-VNode2 (5000) ‚Üí **Server 0**
- üîë **user_dave** (5500) ‚Üí Next server clockwise: üîµ S1-VNode2 (6000) ‚Üí **Server 1**
- üîë **user_eve** (6800) ‚Üí Next server clockwise: üü† S2-VNode2 (7000) ‚Üí **Server 2**
- üîë **user_frank** (8500) ‚Üí Next server clockwise: üü¢ S0-VNode3 (9000) ‚Üí **Server 0**
- üîë **user_grace** (9500) ‚Üí Next server clockwise: üîµ S1-VNode3 (10000) ‚Üí **Server 1**
- üîë **user_henry** (10500) ‚Üí Next server clockwise: üü† S2-VNode3 (11000) ‚Üí **Server 2**

**Result:** Keys evenly distributed across all physical servers!

**Virtual Nodes:** Each physical server gets multiple positions on the ring (typically 100-200)

### Consistent Hashing Algorithm

```mermaid
sequenceDiagram
    participant Client
    participant HashRing
    participant Server

    Client->>HashRing: hash(key) = 6500
    HashRing->>HashRing: Find next server clockwise
    Note over HashRing: Servers at: 1000, 5000, 9000<br/>6500 ‚Üí next is 9000
    HashRing->>Server: Route to Server 2 (9000)
    Server-->>Client: Return data
```

### Real-World Use Cases

```mermaid
mindmap
  root((Consistent<br/>Hashing<br/>Use Cases))
    Distributed Caching
      Memcached
      Redis Cluster
      CDN nodes
    Load Balancing
      Service mesh routing
      API gateway
      Database sharding
    Distributed Storage
      Cassandra
      DynamoDB
      Riak
    Content Distribution
      P2P networks
      BitTorrent
      IPFS
```

### Consistent Hashing vs Simple Hashing

| Aspect | Simple Hash (key % N) | Consistent Hash |
|--------|----------------------|-----------------|
| **Redistribution on add/remove** | ~100% keys remapped | Only K/N keys (K=total keys, N=nodes) |
| **Load distribution** | Perfect if N is stable | Good with virtual nodes |
| **Complexity** | O(1) | O(log N) with binary search |
| **Use case** | Static server count | Dynamic scaling |
| **Cache hit rate on scale** | Drops to ~0% | Maintains ~(N-1)/N hit rate |

### Implementation Pseudocode

```mermaid
graph TD
    Start[Add Key to Ring] --> Hash[Hash key to position P]
    Hash --> Search[Binary search for next server > P]
    Search --> Found{Server found?}
    Found -->|Yes| Return[Return server]
    Found -->|No| Wrap[Wrap around to first server]
    Wrap --> Return

    AddServer[Add Server] --> HashServer[Hash server ID multiple times]
    HashServer --> VNodes[Create 150 virtual nodes]
    VNodes --> Insert[Insert all positions into ring]
    Insert --> Sort[Sort ring by position]

    style Start fill:#9cf
    style AddServer fill:#9f9
```

---

## 2. Stateless vs Stateful Architecture

### Stateful Architecture

```mermaid
graph TB
    Client1[Client A] --> LB[Load Balancer]
    Client2[Client B] --> LB

    LB -->|Session 1| S1[Server 1<br/>üíæ Session Data:<br/>User A logged in<br/>Cart: 3 items]
    LB -->|Session 2| S2[Server 2<br/>üíæ Session Data:<br/>User B logged in<br/>Cart: 1 item]

    Client1 -.->|Next request<br/>MUST go to Server 1| S1

    style S1 fill:#f99
    style S2 fill:#f99
```

**Problem:** Client is "stuck" to specific server (sticky sessions)

### Stateless Architecture

```mermaid
graph TB
    Client1[Client A] --> LB[Load Balancer]
    Client2[Client B] --> LB

    LB -->|Any request| S1[Server 1<br/>No local state]
    LB -->|Any request| S2[Server 2<br/>No local state]
    LB -->|Any request| S3[Server 3<br/>No local state]

    S1 & S2 & S3 --> Cache[(Redis Cache<br/>All session data)]
    S1 & S2 & S3 --> DB[(Database<br/>Persistent data)]

    style S1 fill:#9f9
    style S2 fill:#9f9
    style S3 fill:#9f9
    style Cache fill:#9cf
```

**Benefit:** Any server can handle any request - perfect for horizontal scaling!

### Session Storage Comparison

```mermaid
graph LR
    subgraph "Stateful: Server-Side Sessions"
        SF1[Server 1<br/>Session Store]
        SF2[Server 2<br/>Session Store]
        SF3[Server 3<br/>Session Store]

        Problem[‚ùå Session affinity required<br/>‚ùå Lost on server failure<br/>‚ùå Hard to scale]
    end

    subgraph "Stateless: External Session Store"
        SL1[Server 1]
        SL2[Server 2]
        SL3[Server 3]

        SL1 & SL2 & SL3 --> Redis[(Redis Cluster)]

        Benefit[‚úÖ No session affinity<br/>‚úÖ Survives server failure<br/>‚úÖ Easy to scale]
    end

    style SF1 fill:#f99
    style SF2 fill:#f99
    style SF3 fill:#f99
    style SL1 fill:#9f9
    style SL2 fill:#9f9
    style SL3 fill:#9f9
```

### Stateless Authentication Flow

```mermaid
sequenceDiagram
    participant Client
    participant LB as Load Balancer
    participant S1 as Server 1
    participant S2 as Server 2
    participant Redis
    participant DB

    Client->>LB: Login (username, password)
    LB->>S1: Route to Server 1
    S1->>DB: Verify credentials
    DB-->>S1: User valid
    S1->>Redis: Store session (session_id, user_data)
    S1-->>Client: Return JWT token

    Note over Client: Next request goes to different server

    Client->>LB: Get cart (JWT token)
    LB->>S2: Route to Server 2
    S2->>S2: Validate JWT signature
    S2->>Redis: Get session data
    Redis-->>S2: User data
    S2->>DB: Fetch cart
    DB-->>S2: Cart items
    S2-->>Client: Cart data
```

### Stateless Design Patterns

```mermaid
mindmap
  root((Stateless<br/>Patterns))
    Client-Side State
      JWT tokens
      Cookies
      LocalStorage
      URL parameters
    Shared State Store
      Redis/Memcached
      DynamoDB
      Cassandra
    Database Sessions
      PostgreSQL
      MySQL
      MongoDB
    Distributed Cache
      Redis Cluster
      Hazelcast
      Apache Ignite
```

### State Migration Strategy

```mermaid
graph TD
    Start[Stateful Application] --> Identify[Identify session data]
    Identify --> Choose{Choose storage}

    Choose -->|Fast, temporary| Redis[External Cache<br/>Redis/Memcached]
    Choose -->|Persistent| DB[Database<br/>PostgreSQL/DynamoDB]
    Choose -->|Client-side| JWT[JWT Tokens<br/>Signed by server]

    Redis --> Implement[Implement session API]
    DB --> Implement
    JWT --> Implement

    Implement --> Test[Test with load balancer]
    Test --> Verify{Sessions work<br/>across servers?}

    Verify -->|No| Debug[Debug session sync]
    Verify -->|Yes| Deploy[Deploy stateless architecture]

    Debug --> Test

    style Start fill:#f99
    style Deploy fill:#9f9
```

### Comparison Table

| Aspect | Stateful | Stateless |
|--------|----------|-----------|
| **Scaling** | Vertical only (complex horizontal) | Easy horizontal scaling |
| **Load balancing** | Requires sticky sessions | Any server handles any request |
| **Failure handling** | Lost sessions on crash | Sessions survive server failures |
| **Deployment** | Complex (session drain) | Simple (just add/remove servers) |
| **Memory usage** | High (stores all sessions) | Low (compute only) |
| **Database load** | Lower (cached locally) | Higher (external lookups) |
| **Latency** | Lower (local access) | Slightly higher (network call) |
| **Use cases** | WebSockets, gaming servers | REST APIs, microservices |

---

## 3. Load Balancer Types

### Layer 4 (Transport Layer) Load Balancer

```mermaid
graph TB
    Client[Client<br/>IP: 1.2.3.4<br/>Port: 50000] -->|TCP packet| L4[Layer 4 LB<br/>Works with:<br/>IP + Port only]

    L4 -->|Forward based on<br/>IP hash or round-robin| S1[Server 1<br/>10.0.1.10:80]
    L4 --> S2[Server 2<br/>10.0.1.11:80]
    L4 --> S3[Server 3<br/>10.0.1.12:80]

    Note1[No HTTP inspection<br/>Fast & efficient<br/>NAT + Forwarding]

    style L4 fill:#9cf
```

**Characteristics:**
- Works at TCP/UDP level
- Inspects: IP address, Port
- Fast (no HTTP parsing)
- Cannot route based on URL or headers

### Layer 7 (Application Layer) Load Balancer

```mermaid
graph TB
    Client[Client Request<br/>GET /api/users<br/>Host: api.example.com<br/>Cookie: session=abc] -->|HTTP request| L7[Layer 7 LB<br/>Inspects:<br/>URL, Headers,<br/>Cookies, Body]

    L7 -->|/api/users*| API[API Servers]
    L7 -->|/static/*| Static[Static Servers]
    L7 -->|/admin/*| Admin[Admin Servers]

    API --> A1[API Server 1]
    API --> A2[API Server 2]

    Static --> S1[CDN Server 1]
    Static --> S2[CDN Server 2]

    Admin --> Ad1[Admin Server 1]

    style L7 fill:#f9f
```

**Characteristics:**
- Works at HTTP/HTTPS level
- Inspects: URL, headers, cookies, body
- Slower (parses HTTP)
- Can route based on content

### L4 vs L7 Comparison

```mermaid
graph LR
    subgraph "Layer 4: Fast but Simple"
        L4[L4 Load Balancer]
        L4 -->|Based on IP:Port| L4_1[Backend Pool]

        L4_Features[‚úÖ Very fast<br/>‚úÖ Low latency<br/>‚úÖ Protocol agnostic<br/>‚ùå No content routing<br/>‚ùå No SSL termination<br/>‚ùå No caching]
    end

    subgraph "Layer 7: Slower but Smart"
        L7[L7 Load Balancer]
        L7 -->|Based on URL/Headers| L7_1[Route /api]
        L7 -->|Based on URL/Headers| L7_2[Route /static]

        L7_Features[‚úÖ Content-based routing<br/>‚úÖ SSL termination<br/>‚úÖ Caching<br/>‚úÖ WAF integration<br/>‚ùå Slower<br/>‚ùå Higher latency]
    end

    style L4 fill:#9cf
    style L7 fill:#f9f
```

### Deployment Strategies

#### Strategy 1: DNS Round Robin

**Yes! DNS can do load balancing by returning multiple IPs in rotating order**

```mermaid
sequenceDiagram
    participant C1 as Client 1
    participant C2 as Client 2
    participant C3 as Client 3
    participant DNS
    participant S1 as Server 1.2.3.4
    participant S2 as Server 1.2.3.5
    participant S3 as Server 1.2.3.6

    C1->>DNS: Query: example.com?
    DNS-->>C1: Response: [1.2.3.4, 1.2.3.5, 1.2.3.6]
    C1->>S1: Connect to first IP

    C2->>DNS: Query: example.com?
    DNS-->>C2: Response: [1.2.3.5, 1.2.3.6, 1.2.3.4] (rotated!)
    C2->>S2: Connect to first IP

    C3->>DNS: Query: example.com?
    DNS-->>C3: Response: [1.2.3.6, 1.2.3.4, 1.2.3.5] (rotated!)
    C3->>S3: Connect to first IP
```

**How DNS Round Robin Works:**

```mermaid
graph TB
    subgraph "DNS Server Configuration"
        Config["example.com A records:<br/>1.2.3.4<br/>1.2.3.5<br/>1.2.3.6<br/>TTL: 60 seconds"]
    end

    subgraph "Query 1"
        Q1[Client asks:<br/>example.com] --> R1[DNS returns:<br/>1.2.3.4 ‚Üê first<br/>1.2.3.5<br/>1.2.3.6]
    end

    subgraph "Query 2"
        Q2[Client asks:<br/>example.com] --> R2[DNS returns:<br/>1.2.3.5 ‚Üê first<br/>1.2.3.6<br/>1.2.3.4]
    end

    subgraph "Query 3"
        Q3[Client asks:<br/>example.com] --> R3[DNS returns:<br/>1.2.3.6 ‚Üê first<br/>1.2.3.4<br/>1.2.3.5]
    end

    Config --> Q1 & Q2 & Q3

    style Config fill:#9cf
```

**Problems with DNS Round Robin:**

```mermaid
graph TD
    Issue1[‚ùå No Health Checks] --> Explain1[DNS keeps returning<br/>dead server IPs]
    Issue2[‚ùå Caching] --> Explain2[Browsers/ISPs cache DNS<br/>for minutes/hours<br/>ignores TTL]
    Issue3[‚ùå Uneven Load] --> Explain3[Some clients make<br/>1000x more requests<br/>than others]
    Issue4[‚ùå Geographic Routing] --> Explain4[Cannot route users<br/>to nearest server]

    style Issue1 fill:#f99
    style Issue2 fill:#f99
    style Issue3 fill:#f99
    style Issue4 fill:#f99
```

**When DNS Round Robin Works:**
- ‚úÖ Simple CDN distribution
- ‚úÖ Low-traffic sites
- ‚úÖ Stateless applications
- ‚úÖ When servers rarely fail

**Real Example:**
```bash
# Try this command - you'll see multiple IPs!
$ nslookup google.com

# Returns different IPs each time (round robin):
# 142.250.185.206
# 142.250.185.238
# 142.250.185.174
# ... etc
```

#### Strategy 2: Single Load Balancer

```mermaid
graph TB
    Client[Clients] --> LB[Load Balancer<br/>Single Point of Failure!]

    LB --> S1[Server 1]
    LB --> S2[Server 2]
    LB --> S3[Server 3]

    Problem[‚ùå SPOF<br/>‚ùå Limited capacity<br/>‚úÖ Simple setup]

    style LB fill:#f99
```

#### Strategy 3: Active-Passive LB

```mermaid
graph TB
    Client[Clients] --> VIP[Virtual IP<br/>10.0.0.100]

    VIP --> Active[Active LB<br/>Handles traffic<br/>Heartbeat every 1s]

    Active -.->|Heartbeat| Passive[Passive LB<br/>Standby mode]

    Passive -.->|Takes over if<br/>Active fails| VIP

    Active --> S1[Server 1]
    Active --> S2[Server 2]
    Active --> S3[Server 3]

    style Active fill:#9f9
    style Passive fill:#9cf
```

#### Strategy 4: Active-Active LB (Best)

```mermaid
graph TB
    Client[Clients] --> DNS[DNS<br/>Round-robin]

    DNS --> LB1[Load Balancer 1<br/>Handles 50% traffic]
    DNS --> LB2[Load Balancer 2<br/>Handles 50% traffic]

    LB1 --> Pool[Server Pool]
    LB2 --> Pool

    Pool --> S1[Server 1]
    Pool --> S2[Server 2]
    Pool --> S3[Server 3]
    Pool --> S4[Server 4]

    Benefits[‚úÖ No SPOF<br/>‚úÖ Better capacity<br/>‚úÖ Both LBs utilized]

    style LB1 fill:#9f9
    style LB2 fill:#9f9
```

### Multi-Tier Load Balancing

```mermaid
graph TB
    Internet[Internet] --> DNS[DNS<br/>Global Load Balancer]

    DNS -->|Geographic routing| US[US Region]
    DNS -->|Geographic routing| EU[EU Region]
    DNS -->|Geographic routing| ASIA[Asia Region]

    US --> US_L4[L4 Load Balancer<br/>AWS NLB]
    EU --> EU_L4[L4 Load Balancer<br/>AWS NLB]

    US_L4 --> US_L7[L7 Load Balancer<br/>AWS ALB]
    EU_L4 --> EU_L7[L7 Load Balancer<br/>AWS ALB]

    US_L7 -->|/api/*| US_API[API Servers]
    US_L7 -->|/static/*| US_CDN[CDN Servers]

    EU_L7 -->|/api/*| EU_API[API Servers]
    EU_L7 -->|/static/*| EU_CDN[CDN Servers]

    style DNS fill:#9cf
    style US_L4 fill:#9cf
    style EU_L4 fill:#9cf
    style US_L7 fill:#f9f
    style EU_L7 fill:#f9f
```

### Load Balancer Algorithms Deep Dive

```mermaid
graph TD
    Start{Select Algorithm}

    Start -->|Simple| RR[Round Robin<br/>S1‚ÜíS2‚ÜíS3‚ÜíS1...]
    Start -->|Weighted| WRR[Weighted Round Robin<br/>S1: 3x, S2: 2x, S3: 1x]
    Start -->|Performance| LC[Least Connections<br/>Route to least busy]
    Start -->|Geographic| GEO[Geographic<br/>Route to nearest]
    Start -->|Sticky| HASH[Consistent Hash<br/>Same client ‚Üí Same server]
    Start -->|Smart| AI[Least Response Time<br/>Track actual latency]

    RR --> Use1[Use: Equal capacity servers<br/>Simple workloads]
    WRR --> Use2[Use: Different server sizes<br/>e.g., 2x capacity server]
    LC --> Use3[Use: Long-lived connections<br/>WebSockets, databases]
    GEO --> Use4[Use: Global users<br/>Latency-sensitive apps]
    HASH --> Use5[Use: Session affinity needed<br/>Stateful apps]
    AI --> Use6[Use: Variable backend latency<br/>Optimal performance]

    style RR fill:#9f9
    style WRR fill:#9cf
    style LC fill:#ff9
    style HASH fill:#f9c
```

### Health Checks

```mermaid
sequenceDiagram
    participant LB as Load Balancer
    participant S1 as Server 1 (Healthy)
    participant S2 as Server 2 (Unhealthy)
    participant S3 as Server 3 (Healthy)

    loop Every 5 seconds
        LB->>S1: GET /health
        S1-->>LB: 200 OK

        LB->>S2: GET /health
        S2-->>LB: Timeout / 500 Error

        LB->>S3: GET /health
        S3-->>LB: 200 OK
    end

    Note over LB,S2: S2 marked unhealthy<br/>Removed from rotation

    LB->>S1: Route traffic
    LB->>S3: Route traffic
    LB-xS2: No traffic sent

    Note over LB,S2: After 3 successful checks,<br/>S2 added back to rotation
```

### Popular Load Balancers

```mermaid
mindmap
  root((Load<br/>Balancers))
    Hardware
      F5 BIG-IP
      Citrix ADC
      A10 Networks
      Expensive, powerful
    Software
      HAProxy
      NGINX
      Envoy
      Traefik
      Free, flexible
    Cloud
      AWS ELB/ALB/NLB
      GCP Load Balancer
      Azure Load Balancer
      Cloudflare
      Managed, scalable
    Service Mesh
      Istio
      Linkerd
      Consul
      App-level routing
```

---

## Quick Reference Tables

### When to Use What?

| Scenario | Solution | Why? |
|----------|----------|------|
| **Caching layer with 100 servers** | Consistent Hashing | Minimize cache invalidation on scale |
| **REST API servers** | Stateless + L7 LB | Easy scaling, content routing |
| **WebSocket gaming server** | Stateful + L4 LB + Sticky sessions | Maintain connection state |
| **Global CDN** | Multi-tier LB + Geo routing | Low latency worldwide |
| **Database connection pool** | Consistent Hashing | Reuse connections per shard |
| **Microservices routing** | Stateless + Service Mesh | Dynamic routing, observability |

### Technology Stack Examples

| Component | Layer 4 | Layer 7 | Stateless Store |
|-----------|---------|---------|-----------------|
| **AWS** | Network Load Balancer (NLB) | Application Load Balancer (ALB) | ElastiCache Redis |
| **GCP** | TCP/UDP Load Balancer | HTTP(S) Load Balancer | Cloud Memorystore |
| **Azure** | Load Balancer | Application Gateway | Azure Cache for Redis |
| **Open Source** | HAProxy (TCP mode) | NGINX, HAProxy (HTTP mode) | Redis Cluster |

---

## Practice Problems

1. **Design Instagram's feed cache**: Use consistent hashing to distribute user feeds across 500 cache servers
2. **Design Uber's matching service**: Should it be stateful or stateless? Why?
3. **Design a global API**: Use multi-tier load balancing (DNS ‚Üí L4 ‚Üí L7)
4. **Migrate from stateful to stateless**: Plan the migration for a monolith with server-side sessions

## Resources
- [Consistent Hashing Paper](https://www.akamai.com/us/en/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf)
- [HAProxy Documentation](http://www.haproxy.org/)
- [NGINX Load Balancing Guide](https://docs.nginx.com/nginx/admin-guide/load-balancer/)

## Reflections
Consistent hashing is brilliant - it's all about minimizing the blast radius when systems change. The virtual nodes trick ensures even distribution. Stateless architecture is the foundation of modern cloud-native apps - any server can die without data loss. Layer 7 load balancers are slower but give you powerful routing capabilities.

## Next Steps
- [ ] Implement consistent hashing in code
- [ ] Draw architecture for Twitter's timeline cache with consistent hashing
- [ ] Compare AWS ALB vs NLB for a specific use case
- [ ] Design session management for a stateless e-commerce app

---

[‚Üê Back to Daily Logs](index.md) | [Home](../index.md)
